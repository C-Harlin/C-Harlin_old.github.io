<!DOCTYPE html>





<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
<meta name="google-site-verification" content="dzYLIgX3Uh7J9m2ciq7XYYQDdCZrsZxnNfwCQAb7ozs" />
  <link rel="apple-touch-icon" sizes="180x180" href="/images/icon_2y_192.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/icon_2y_32.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/icon_2y_24.png?v=7.4.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.4.0" color="#222">
  <meta name="google-site-verification" content="googleab89012a7e7f3666.html">

<link rel="stylesheet" href="/css/main.css?v=7.4.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.4.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="沿着Attention的发展脉络挑了三篇论文，成。">
<meta name="keywords" content="机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习中的注意力机制&#x2F;Attention Mechanism">
<meta property="og:url" content="https://C-Harlin.github.io/2019/11/22/深度学习中的注意力机制/index.html">
<meta property="og:site_name" content="Harlin&#39;Blog">
<meta property="og:description" content="沿着Attention的发展脉络挑了三篇论文，成。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://raw.githubusercontent.com/C-Harlin/MarkDownPhotos/master/attention/seq2seq.png">
<meta property="og:image" content="https://raw.githubusercontent.com/C-Harlin/MarkDownPhotos/master/attention/Bahdanau.png">
<meta property="og:image" content="https://raw.githubusercontent.com/C-Harlin/MarkDownPhotos/master/attention/luong2015.png">
<meta property="og:image" content="https://raw.githubusercontent.com/C-Harlin/MarkDownPhotos/master/attention/transformer.png">
<meta property="og:image" content="https://raw.githubusercontent.com/C-Harlin/MarkDownPhotos/master/attention/embedding.png">
<meta property="og:image" content="https://raw.githubusercontent.com/C-Harlin/MarkDownPhotos/master/attention/multi-head-attention.png">
<meta property="og:image" content="https://raw.githubusercontent.com/C-Harlin/MarkDownPhotos/master/attention/transformer_multi-headed_self-attention-recap.png">
<meta property="og:updated_time" content="2019-12-25T14:26:28.187Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习中的注意力机制&#x2F;Attention Mechanism">
<meta name="twitter:description" content="沿着Attention的发展脉络挑了三篇论文，成。">
<meta name="twitter:image" content="https://raw.githubusercontent.com/C-Harlin/MarkDownPhotos/master/attention/seq2seq.png">
  <link rel="canonical" href="https://C-Harlin.github.io/2019/11/22/深度学习中的注意力机制/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>深度学习中的注意力机制/Attention Mechanism | Harlin'Blog</title>
  
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-150644916-1"></script>
  <script>
    var host = window.location.hostname;
    if (host !== "localhost" || !true) {
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-150644916-1');
    }
  </script>








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Harlin'Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
      <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block post">
    <link itemprop="mainEntityOfPage" href="https://C-Harlin.github.io/2019/11/22/深度学习中的注意力机制/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Harlin">
      <meta itemprop="description" content="Everyday counts">
      <meta itemprop="image" content="/images/dog.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Harlin'Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">深度学习中的注意力机制/Attention Mechanism

          
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2019-11-22 10:10:02" itemprop="dateCreated datePublished" datetime="2019-11-22T10:10:02+08:00">2019-11-22</time>
            </span>
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/学习笔记/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a></span>

                
                
              
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>沿着Attention的发展脉络挑了三篇论文，成。 <a id="more"></a></p>
<h2 id="背景">背景</h2>
<p>一切都要从<strong>Seq2Seq</strong>模型说起。<br></p>
<p>Seq2Seq模型（<a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" target="_blank" rel="noopener">Sutskever, et al. 2014</a>）最初是为机器翻译而提出。它使用LSTM将输入序列编码为一个定长向量，然后使用另一个LSTM从该向量中解码出目标序列，在形式上表现为encoder-decoder的结构。 <img src="https://raw.githubusercontent.com/C-Harlin/MarkDownPhotos/master/attention/seq2seq.png" width="80%" height="80%"> <em>图1 Seq2Seq模型用于机器翻译的示例。图片来源：<a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#whats-wrong-with-seq2seq-model" target="_blank" rel="noopener">Lilian's Blog</a></em><br> 但是Seq2Seq模型的问题在于，它试图将整个句子压缩为一个向量表示，因此一旦句子过长，压缩向量就会“记不住”那么长的句子，导致模型的效果随着句子长度的增加而急剧变差。</p>
<h2 id="注意力机制的提出">注意力机制的提出</h2>
<p>针对Seq2Seq模型的问题， Bahdanau等人第一次提出了注意力机制（<a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau et al., 2015</a>）。有别于Seq2Seq模型将整个句子压缩为一个向量再去解码的做法，注意力机制的核心是使用上下文向量（context vector）来生成输出序列。其中上下文向量基于encoder和decoder的隐向量（hidden state）得到。</p>
<p><img src="https://raw.githubusercontent.com/C-Harlin/MarkDownPhotos/master/attention/Bahdanau.png" width="80%" height="80%"> <em><a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau et al., 2015</a>使用的模型结构</em><br> 首先，decoder的输出是联合条件概率模型： <span class="math display">\[ p(\mathbf{y})=\prod_{t=1}^{T} p\left(y_{t} |\left\{y_{1}, \cdots, y_{t-1}\right\}, c_{t}\right) \tag{1.1} \]</span> 其中，<span class="math inline">\(\mathbf{y}=\left(y_{1}, \cdots, y_{T_y}\right)\)</span>是输出的句子，<span class="math inline">\(c_{t}\)</span>是上下文向量。联合条件概率中的每一个条件概率可以被建模成为： <span class="math display">\[ p\left(y_{i} |\left\{y_{1}, \cdots, y_{i-1}\right\}, c_{i}\right)=g\left(y_{i-1}, s_{i}, c_{i}\right) \tag{1.2} \]</span> 其中<span class="math inline">\(g\)</span>是非线性函数，用来输出单词<span class="math inline">\(y_t\)</span>的作为结果的概率，<span class="math inline">\(s_t\)</span>则是decoder RNN的隐向量： <span class="math display">\[ s_{i}=f\left(s_{i-1}, y_{i-1}, c_{i}\right) \tag{1.3} \]</span> 上下文向量<span class="math inline">\(c_{i}\)</span>是encoder中所有隐向量<span class="math inline">\(h\)</span>的加权和： <span class="math display">\[ c_{i}=\sum_{j=1}^{T_{x}} \alpha_{i j} h_{j} \tag{1.4} \]</span> 每个隐向量<span class="math inline">\(h_j\)</span>的权重<span class="math inline">\(\alpha_{i j}\)</span>通过softmax计算而得： <span class="math display">\[ \alpha_{i j}=\frac{\exp \left(e_{i j}\right)}{\sum_{k=1}^{T_{x}} \exp \left(e_{i k}\right)} \tag{1.5} \]</span> 其中<span class="math display">\[ e_{i j}=a\left(s_{i-1}, h_{j}\right) \tag{1.6} \]</span> 这里的<span class="math inline">\(a\)</span>是一个前向传播神经网络，用来计算在位置<span class="math inline">\(j\)</span>附近的输入序列和在位置<span class="math inline">\(i\)</span>附近的输出序列的匹配程度。</p>
<p>从公式<span class="math inline">\((1.4)\)</span>中可以看出，由于上下文向量<span class="math inline">\(c\)</span>用到了整个输入序列，因此就解决了因句子过长导致的遗忘问题。概率<span class="math inline">\(\alpha_{i j}\)</span>反映的是隐向量<span class="math inline">\(h_j\)</span>对于决定下一个状态<span class="math inline">\(s_i\)</span>并生成<span class="math inline">\(y_i\)</span>的重要程度，也正是在这里体现了“注意力”，即decoder只需要“注意”输入序列中的某一部分——不太重要的部分权重小一点，重要的部分权重大一点。通过这一注意力机制，encoder便不再需要将整个长句的信息压缩到一个定长向量中。</p>
<h2 id="全局-vs-局部注意力global-vs-local-attention">全局 vs 局部注意力/Global vs Local attention</h2>
<p>紧接着，<a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong, et al., 2015</a>提出了“全局（Global）”和“局部（Local）”注意力。 <img src="https://raw.githubusercontent.com/C-Harlin/MarkDownPhotos/master/attention/luong2015.png" width="80%" height="80%"> - Global attention</p>
<p>和<a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau et al., 2015</a>中的方法很相似，但做了如下改动： 1. encoder使用了单向RNN，而<a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau et al., 2015</a>使用的是双向RNN。这样做的好处是省去了组合两个方向上隐向量的工作； 2. <span class="math inline">\(h_t\rightarrow a_t\rightarrow c_t\rightarrow \tilde{h}_t\rightarrow y_t\)</span>，即decoder的隐向量是按照常规RNN的方法计算得到的，<span class="math inline">\(e.g.,\ h_t=tanh(W_{hh}h_{t-1}+W_{xh}x_t)\)</span>，之后再去计算上下文向量 ；而<a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau et al., 2015</a>是用上一步的隐向量计算出上下文向量后，再在此基础上计算当前步的隐向量<span class="math inline">\(h_{t-1}\rightarrow a_t\rightarrow c_t\rightarrow h_t\rightarrow y_t\)</span>。两者的逻辑顺序有些许不同； 3. 提出了多种方法来计算注意力的权重。 具体来说，<br> <span class="math inline">\(h_t\rightarrow a_t\)</span>，即利用decoder和encoder的隐向量计算权重向量： <span class="math display">\[ \begin{aligned} \boldsymbol{a}_{t}(s) &amp;=\operatorname{align}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s}\right) \\ &amp;=\frac{\exp \left(\operatorname{score}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s}\right)\right)}{\sum_{s^{\prime}} \exp \left(\operatorname{score}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s^{\prime}}\right)\right)} \end{aligned} \]</span> 这一步与<a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau et al., 2015</a>在方式上没有区别，都是使用了softmax。唯一的不同之处是<a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">Bahdanau et al., 2015</a>使用的是<span class="math inline">\(h_{t-1}\)</span>。（其中<span class="math inline">\(\overline{\boldsymbol{h}}_{s}\)</span>是encoder的隐向量）<br> <span class="math inline">\(a_t\rightarrow c_t\)</span>，即利用权重向量和encoder的隐向量计算上下文向量：这一步没什么意外，向量做内积即可。<br> <span class="math inline">\(c_t\rightarrow \tilde{h}_t\rightarrow y_t\)</span>，即利用上下文向量计算注意力隐向量<span class="math inline">\(\tilde{\boldsymbol{h}}_{t}\)</span>和预测输出<span class="math inline">\(p\left(y_{t} | y_{\lt t}, x\right)\)</span> <span class="math display">\[ \begin{aligned}&amp;\tilde{\boldsymbol{h}}_{t}=\tanh \left(\boldsymbol{W}_{c}\left[\boldsymbol{c}_{t} ; \boldsymbol{h}_{t}\right]\right) \\ &amp;p\left(y_{t} | y_{\lt t}, x\right)=\operatorname{softmax}\left(\boldsymbol{W}_{s}\tilde{\boldsymbol{h}}_{t}\right) \end{aligned} \]</span> - Local attention 然而Global attention的一个缺陷是，在计算每个输出时都用到了encoder中的所有词语。一旦句子过长，例如段落或是文章，那么这种方法就会显得非常低效。因此<a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Luong, et al., 2015</a>接着提出了Local attention，只使用源语句中的部分词语产生输出。其灵感的来源是<a href="http://proceedings.mlr.press/v37/xuc15.pdf" target="_blank" rel="noopener">Xu et al. 2015</a>针对图像领域提出的soft &amp; hard attention。 具体来说，Local attention首先为<span class="math inline">\(t\)</span>时刻的输出目标在源语句中预测出一个匹配位置<span class="math inline">\(p_t\)</span>（aligned position），并以<span class="math inline">\(p_t\)</span>为中心选定一个窗口<span class="math inline">\(\left[p_{t}-D, p_{t}+D\right]\)</span>，其中<span class="math inline">\(D\)</span>是超参数。上下文向量<span class="math inline">\(c_t\)</span>仅通过该窗口内encoder的隐向量的加权和计算而得。 <span class="math display">\[ p_{t}=S \cdot \operatorname{sigmoid}\left(\boldsymbol{v}_{p}^{\top} \tanh \left(\boldsymbol{W}_{\boldsymbol{p}} \boldsymbol{h}_{t}\right)\right) \]</span> <span class="math inline">\(\boldsymbol{W}_{\boldsymbol{p}}\)</span>和<span class="math inline">\(\boldsymbol{v}_{p}^{\top}\)</span>都是模型参数，<span class="math inline">\(S\)</span>是源语句的长度。为了使得更靠近中心的词语的权重更大，又引入了高斯分布为系数来对权重做修正： <span class="math display">\[ \boldsymbol{a}_{t}(s)=\operatorname{align}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s}\right) \exp \left(-\frac{\left(s-p_{t}\right)^{2}}{2 \sigma^{2}}\right) \]</span></p>
<h2 id="transformer">Transformer</h2>
<p>上述方法都不约而同地使用RNN来完成Seq2Seq的任务，这是基于RNN处理序列数据的天然优势。然而，这种优势同时也不幸成为了阻碍RNN在训练过程中并行化的绊脚石。一旦序列过长，有限的内存将会限制样本的批处理。<a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener">“Attention is All you Need”</a>(Vaswani, et al., 2017)提出的transformer模型毫无疑问是第一个只靠self-attention而不使用RNN计算句子输入输出表示的方法。</p>
<p><a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener">Vaswani, et al., 2017</a>文中对<strong>self-attention</strong>的解释仅有一句"所谓的self-attention，又名intra-attention，是一种关联句子中不同位置上的词语来计算句子表示的注意力机制。"，第一次接触这个概念难免令人看得云里雾里。经我查阅资料后，目前对此的理解就是一种不同于RNN那样将源语句依次嵌入成隐向量实现信息融合，而是同时基于源语句中的词语去计算attention。简言之，就是要用前馈神经网络去做之前RNN做的事。 <img src="https://raw.githubusercontent.com/C-Harlin/MarkDownPhotos/master/attention/transformer.png" width="80%" height="80%"> <em>Transformer的框架</em> ### Scale dot-product attention 看多了之前的RNN模型，很容易在此处产生误解。尤其是看到了encoder-decoder的结构，以及下面提到的attention计算公式，会不自觉地想要将transformer和基于RNN的模型对应起来。所以建议直接将这里的设定当做是全新引入的来接受。 首先出现的是三个输入向量，分别是query(Q)、key(K)、value(V)，它们都是由词向量乘以不同的参数矩阵后得到。（在input embedding中完成对词语的向量化，其中key和value作为一对，其维度是相同的），如下图所示： <img src="https://raw.githubusercontent.com/C-Harlin/MarkDownPhotos/master/attention/embedding.png" width="80%" height="80%"> 清楚了输入向量之后，我们直接来看计算attention的计算，即上图中最右侧的图： <span class="math display">\[ \text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V \]</span> 这里就和上述基于RNN的模型有些相似了。 因为计算attention所需的信息是共享的，即（K，V），所以此处的Q表示多个query构成的矩阵，通过矩阵相乘一并快速计算。 （"scale"体现在公式中除以<span class="math inline">\(\sqrt{d_k}\)</span>）</p>
<h3 id="multi-head-self-attention">Multi-Head Self-Attention</h3>
<p><img src="https://raw.githubusercontent.com/C-Harlin/MarkDownPhotos/master/attention/multi-head-attention.png" width="80%" height="80%"> <em>Multi-head scaled dot-product attention mechanism. (图像来源: Fig 2 in Vaswani, et al., 2017)</em> 上述的Scale dot-product attention是一次性计算attention，而Multi-Head Self-Attention则是多次并行计算attention。 具体来说： - 首先利用一组参数矩阵<span class="math inline">\(W^Q,W^K,W^V\)</span>对<span class="math inline">\(Q,K,V\)</span>三个矩阵分别做线性变换，然后再接Scale dot-product attention。线性变化的目的是降维，比如<a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener">Vaswani, et al., 2017</a>文中就将维度从512将至64。由此得到的结果称为<span class="math inline">\(head\)</span>； - 一共有<span class="math inline">\(h\)</span>组经过不同初始化得到的参数矩阵<span class="math inline">\(W^{Q}_i,W^{K}_i,W^{V}_i\)</span>，每一组都做同样的Scale dot-product attention，于是得到<span class="math inline">\(h\)</span>组<span class="math inline">\(head\)</span>，然后将它们级联到一起后再做一次线性变换。 <span class="math display">\[ \begin{aligned} \text { MultiHead }(Q, K, V) &amp;=\text { Concat }\left(\text { head }_{1}, \ldots, \text { head }_{\mathrm{h}}\right) W^{O} \\ \text { where head }_{\mathrm{i}} &amp;=\text { Attention }\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) \end{aligned} \]</span></p>
<p>这里再用一张图示意上述流程： <img src="https://raw.githubusercontent.com/C-Harlin/MarkDownPhotos/master/attention/transformer_multi-headed_self-attention-recap.png" width="80%" height="80%"> 文中说这样做的好处是attention联合了不同的表示子空间，而single-head attention则相当于取了它们的平均。而且文中说这样做和原先的single-head attention相比计算量差不多。</p>
<h3 id="positional-encoding">Positional Encoding</h3>
<p>毕竟语句是序列数据，词语的顺序也是至关重要的信息。transformer使用Positional Encoding来引入顺序的信息。简单来说就是给input embedding加上一个向量，而这个向量可以在一定程度上反映词语在句子中的位置信息。文中是使用正弦函数和余弦函数来完成这一步的，想了解细节还请参考原文。</p>
<h3 id="从encoder到decoder">从encoder到decoder</h3>
<p>从encoder最上层的那个模块到decoder模块的中间层输入其实还是遵循了之前的做法，即通过参数矩阵对encoder的输出做线性变换，得到一组（K，V），而Q则来自与decoder模块的第一层的输出。</p>
<h3 id="decoder">decoder</h3>
<p>和encoder的模块稍有不同，decoder的模块中包含了三个子层，其中第二层和第三层与encoder无异，而第一层略微改变的地方是使用了Masked Multi-Head Attention。 具体来说，在解码器中仅允许self-attention关注输出序列中“过去”的位置。这是通过在self-attention计算softmax步骤之前，先屏蔽掉“将来”的位置（将它们设置为-inf）来完成的。</p>
<h3 id="the-final-linear-and-softmax-layer">The Final Linear and Softmax Layer</h3>
<p>最后就是全连接层加softmax得到输出结果，当然对应的单词作为下次decoder的query。</p>

    </div>

    
    
    
        
      

      <footer class="post-footer">
          
            
          
          <div class="post-tags">
            
              <a href="/tags/机器学习/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a>
            
          </div>
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2019/10/29/你人生中最努力的事情是什么/" rel="next" title="你人生中最努力的事情是什么">
                  <i class="fa fa-chevron-left"></i> 你人生中最努力的事情是什么
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#背景"><span class="nav-number">1.</span> <span class="nav-text">背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#注意力机制的提出"><span class="nav-number">2.</span> <span class="nav-text">注意力机制的提出</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#全局-vs-局部注意力global-vs-local-attention"><span class="nav-number">3.</span> <span class="nav-text">全局 vs 局部注意力/Global vs Local attention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#transformer"><span class="nav-number">4.</span> <span class="nav-text">Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#multi-head-self-attention"><span class="nav-number">4.1.</span> <span class="nav-text">Multi-Head Self-Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#positional-encoding"><span class="nav-number">4.2.</span> <span class="nav-text">Positional Encoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#从encoder到decoder"><span class="nav-number">4.3.</span> <span class="nav-text">从encoder到decoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#decoder"><span class="nav-number">4.4.</span> <span class="nav-text">decoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#the-final-linear-and-softmax-layer"><span class="nav-number">4.5.</span> <span class="nav-text">The Final Linear and Softmax Layer</span></a></li></ol></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/dog.jpg"
      alt="Harlin">
  <p class="site-author-name" itemprop="name">Harlin</p>
  <div class="site-description" itemprop="description">Everyday counts</div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:harlingcao@163.com" title="E-Mail &rarr; mailto:harlingcao@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="http://weibo.com/u/3748819491" title="Weibo &rarr; http://weibo.com/u/3748819491" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a>
      </span>
    
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Harlin</span>
</div>

        












        
      </div>
    </footer>
  </div>

  


  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.4.0"></script><script src="/js/motion.js?v=7.4.0"></script>
<script src="/js/schemes/pisces.js?v=7.4.0"></script>
<script src="/js/next-boot.js?v=7.4.0"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>





















  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

    
  

  

  

</body>
</html>
